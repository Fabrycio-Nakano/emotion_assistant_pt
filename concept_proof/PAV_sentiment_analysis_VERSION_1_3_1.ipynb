{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GltA3w6Iikou"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "!pip install librosa==0.9.0 pydub noisereduce scikit-learn tensorflow torchaudio seaborn matplotlib transformers\n",
        "!sudo apt update && sudo apt install ffmpeg"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "arquivo = \"/content/Gravação233.m4a\"\n",
        "!whisper {arquivo} --model medium --task transcribe --language pt --output_format txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pLCx0dNjdAz",
        "outputId": "99375181-74df-4328-fb71-9986d95efd70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100%|█████████████████████████████████████| 1.42G/1.42G [00:31<00:00, 48.5MiB/s]\n",
            "[00:00.000 --> 00:11.000]  Pessoal, hoje é dia da patrulha do consumidor que mostra uma blitz do Procon e da polícia civil no supermercado denunciado pelos clientes por falta de higiene.\n",
            "[00:11.000 --> 00:19.000]  Durante a fiscalização foram encontrados alimentos vencidos, impróprios para consumo e armazenados de maneira inadequada.\n",
            "[00:19.000 --> 00:21.000]  Acompanhe com atenção a reportagem.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import torch\n",
        "import librosa\n",
        "import torchaudio\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, BatchNormalization\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n",
        "from tensorflow.keras.models import load_model\n",
        "from transformers import pipeline\n",
        "import logging\n"
      ],
      "metadata": {
        "id": "McMlRHeKjf3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
      ],
      "metadata": {
        "id": "BbN-tvyijgkc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProcessadorAudio:\n",
        "    def __init__(self, taxa_amostragem_alvo=16000, n_fft=1024, hop_length=512, n_mels=64, max_pad_len=400):\n",
        "        self.taxa_amostragem_alvo = taxa_amostragem_alvo\n",
        "        self.n_fft = n_fft\n",
        "        self.hop_length = hop_length\n",
        "        self.n_mels = n_mels\n",
        "        self.max_pad_len = max_pad_len\n",
        "\n",
        "    def carregar_audio(self, caminho):\n",
        "        waveform, sample_rate = torchaudio.load(caminho)\n",
        "        if sample_rate != self.taxa_amostragem_alvo:\n",
        "            resample_transform = torchaudio.transforms.Resample(orig_freq=sample_rate, new_freq=self.taxa_amostragem_alvo)\n",
        "            waveform = resample_transform(waveform)\n",
        "        return waveform\n",
        "\n",
        "    def extrair_caracteristicas(self, waveform):\n",
        "        mel_spectrogram_transform = torchaudio.transforms.MelSpectrogram(\n",
        "            sample_rate=self.taxa_amostragem_alvo,\n",
        "            n_fft=self.n_fft,\n",
        "            hop_length=self.hop_length,\n",
        "            n_mels=self.n_mels\n",
        "        )\n",
        "        mel_spectrogram = mel_spectrogram_transform(waveform)\n",
        "        mel_spectrogram = mel_spectrogram.mean(dim=0)\n",
        "        pad = self.max_pad_len - mel_spectrogram.shape[-1]\n",
        "        if pad > 0:\n",
        "            mel_spectrogram = torch.nn.functional.pad(mel_spectrogram, (0, pad))\n",
        "        else:\n",
        "            mel_spectrogram = mel_spectrogram[:, :self.max_pad_len]\n",
        "        return mel_spectrogram.numpy().flatten()"
      ],
      "metadata": {
        "id": "qBK7ZHPTjmOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DetectorEmocao:\n",
        "    def __init__(self, modelo):\n",
        "        self.modelo = modelo\n",
        "\n",
        "    def prever(self, caracteristicas):\n",
        "        caracteristicas = caracteristicas[np.newaxis, ..., np.newaxis]\n",
        "        previsoes = self.modelo.predict(caracteristicas)\n",
        "        return np.argmax(previsoes, axis=1)[0]"
      ],
      "metadata": {
        "id": "rLnH0d3bjm00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ProcessadorNLP:\n",
        "    def __init__(self):\n",
        "        self.analisador_sentimento = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
        "\n",
        "    def analisar_sentimento(self, texto):\n",
        "        resultado = self.analisador_sentimento(texto)\n",
        "        sentimento = resultado[0]['label']  # mapeando as saídas do modelo de NLP para as emotions esperadas\n",
        "\n",
        "        mapeamento_sentimentos = {\n",
        "            \"1 star\": \"sad\",\n",
        "            \"2 stars\": \"neutral\",\n",
        "            \"3 stars\": \"neutral\",\n",
        "            \"4 stars\": \"happy\",\n",
        "            \"5 stars\": \"happy\"\n",
        "        }\n",
        "        return mapeamento_sentimentos.get(sentimento, \"neutral\")"
      ],
      "metadata": {
        "id": "cVMCBzc0jpPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GeradorResposta:\n",
        "    def __init__(self):\n",
        "        self.respostas = {\n",
        "            \"happy\": [\"Que bom te ver feliz!\", \"Continue sorrindo!\"],\n",
        "            \"sad\": [\"Lamento ouvir isso.\", \"Estou aqui para ajudar.\"],\n",
        "            \"angry\": [\"Por favor, respire fundo.\", \"Vamos tentar se acalmar.\"],\n",
        "            \"neutral\": [\"Como posso te ajudar hoje?\", \"O que gostaria de fazer?\"]\n",
        "        }\n",
        "\n",
        "    def gerar_resposta(self, emocao_audio, emocao_texto): # Combina a emoção detectada no audio e texto para gerar uma resposta mais contextualizada\n",
        "        if emocao_texto == \"neutral\":\n",
        "            return np.random.choice(self.respostas[emocao_audio])\n",
        "        return np.random.choice(self.respostas[emocao_texto])\n"
      ],
      "metadata": {
        "id": "S4a1upsjjsaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AssistenteAudioEmocional:\n",
        "    def __init__(self, processador_audio, detector_emocao, processador_nlp, gerador_resposta):\n",
        "        self.processador_audio = processador_audio\n",
        "        self.detector_emocao = detector_emocao\n",
        "        self.processador_nlp = processador_nlp\n",
        "        self.gerador_resposta = gerador_resposta\n",
        "        self.buffer = []\n",
        "\n",
        "    def processar_audio(self, caminho_audio):\n",
        "        logging.info(\"Processando áudio...\")\n",
        "        waveform = self.processador_audio.carregar_audio(caminho_audio)\n",
        "        caracteristicas = self.processador_audio.extrair_caracteristicas(waveform)\n",
        "        emocao = self.detector_emocao.prever(caracteristicas)\n",
        "        return emocao\n",
        "\n",
        "    def transcrever_audio(self, caminho_audio):\n",
        "        logging.info(\"Transcrevendo áudio...\")\n",
        "        !whisper {caminho_audio} --model medium --task transcribe --language pt --output_format txt\n",
        "        with open(caminho_audio.replace('.m4a', '.txt'), 'r') as file:\n",
        "            transcricao = file.read()\n",
        "        return transcricao\n",
        "\n",
        "    def processar_texto(self, texto):\n",
        "        logging.info(\"Processando texto...\")\n",
        "        sentimento = self.processador_nlp.analisar_sentimento(texto)\n",
        "        return sentimento\n",
        "\n",
        "    def gerar_resposta(self, emocao_audio, emocao_texto):\n",
        "        logging.info(\"Gerando resposta...\")\n",
        "        return self.gerador_resposta.gerar_resposta(emocao_audio, emocao_texto)\n",
        "\n",
        "    def lidar_interacao(self, caminho_audio):\n",
        "        emocao_audio = self.processar_audio(caminho_audio)\n",
        "        transcricao = self.transcrever_audio(caminho_audio)\n",
        "        emocao_texto = self.processar_texto(transcricao)\n",
        "        resposta = self.gerar_resposta(emocao_audio, emocao_texto)\n",
        "        return resposta"
      ],
      "metadata": {
        "id": "5K8fAt2njwAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def construir_modelo_emocao(input_shape, num_classes, caminho_pesos):\n",
        "    modelo = Sequential([\n",
        "        Conv1D(64, 3, activation='relu', padding='same', input_shape=input_shape),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(2),\n",
        "        Dropout(0.2),\n",
        "        Conv1D(128, 3, activation='relu', padding='same'),\n",
        "        BatchNormalization(),\n",
        "        MaxPooling1D(2),\n",
        "        Dropout(0.2),\n",
        "        Flatten(),\n",
        "        Dense(256, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    modelo.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "    if os.path.exists(caminho_pesos):   # logit para verificar se o arquivo de pesos existe\n",
        "        modelo.load_weights(caminho_pesos)\n",
        "        logging.info(f\"Pesos carregados de {caminho_pesos}\")\n",
        "    else:\n",
        "        logging.error(f\"Arquivo de pesos não encontrado: {caminho_pesos}\")\n",
        "    return modelo"
      ],
      "metadata": {
        "id": "zH0M-wbMj0uX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown 1p1S5yO1F77aegNeNe2UpVkN6nZicvm8D"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rudz9FRRrs-D",
        "outputId": "83fc9c08-819c-455e-9c15-9c5840e0dc0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1p1S5yO1F77aegNeNe2UpVkN6nZicvm8D\n",
            "From (redirected): https://drive.google.com/uc?id=1p1S5yO1F77aegNeNe2UpVkN6nZicvm8D&confirm=t&uuid=efb17398-8251-4ee3-b4bc-ac97d7650c16\n",
            "To: /content/best_model_emoUERJ.h5\n",
            "100% 2.52G/2.52G [00:36<00:00, 68.1MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = '/content/best_model_emoUERJ.h5'\n",
        "model = load_model(model_path)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LePPAbfItO_p",
        "outputId": "f94947eb-daa6-4a07-f288-ed0c9f8bc468"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv1d_4 (Conv1D)           (None, 25600, 64)         256       \n",
            "                                                                 \n",
            " batch_normalization_4 (Bat  (None, 25600, 64)         256       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_4 (MaxPoolin  (None, 12800, 64)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 12800, 64)         0         \n",
            "                                                                 \n",
            " conv1d_5 (Conv1D)           (None, 12800, 128)        24704     \n",
            "                                                                 \n",
            " batch_normalization_5 (Bat  (None, 12800, 128)        512       \n",
            " chNormalization)                                                \n",
            "                                                                 \n",
            " max_pooling1d_5 (MaxPoolin  (None, 6400, 128)         0         \n",
            " g1D)                                                            \n",
            "                                                                 \n",
            " dropout_7 (Dropout)         (None, 6400, 128)         0         \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 819200)            0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 256)               209715456 \n",
            "                                                                 \n",
            " dropout_8 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 4)                 1028      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 209742212 (800.10 MB)\n",
            "Trainable params: 209741828 (800.10 MB)\n",
            "Non-trainable params: 384 (1.50 KB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (400 * 64, 1)\n",
        "num_classes = 4  # numero de classes\n",
        "caminho_pesos = '/content/best_model_emoUERJ.h5'\n",
        "modelo_emocao = construir_modelo_emocao(input_shape, num_classes, caminho_pesos)"
      ],
      "metadata": {
        "id": "MdfT6gcOuj__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processador_audio = ProcessadorAudio()\n",
        "detector_emocao = DetectorEmocao(modelo_emocao)\n",
        "processador_nlp = ProcessadorNLP()\n",
        "gerador_resposta = GeradorResposta()\n",
        "\n",
        "assistente = AssistenteAudioEmocional(processador_audio, detector_emocao, processador_nlp, gerador_resposta)"
      ],
      "metadata": {
        "id": "1pElDyV9j6Vf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "caminho_audio = \"/content/Gravação233.m4a\"\n",
        "resposta = assistente.lidar_interacao(caminho_audio)\n",
        "print(\"Resposta do Assistente:\", resposta)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZ1cDFgIj7no",
        "outputId": "8f17134e-3567-4b62-b439-853ef23d8971"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 47ms/step\n",
            "[00:00.000 --> 00:11.000]  Pessoal, hoje é dia da patrulha do consumidor que mostra uma blitz do Procon e da polícia civil no supermercado denunciado pelos clientes por falta de higiene.\n",
            "[00:11.000 --> 00:19.000]  Durante a fiscalização foram encontrados alimentos vencidos, impróprios para consumo e armazenados de maneira inadequada.\n",
            "[00:19.000 --> 00:21.000]  Acompanhe com atenção a reportagem.\n",
            "Resposta do Assistente: Lamento ouvir isso.\n"
          ]
        }
      ]
    }
  ]
}
